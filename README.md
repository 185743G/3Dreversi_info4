# G2「DQN を用いた 3 次元リバーシ AI の特徴の考察」
## 概要
本文書は強化学習の中でも DQN(Deep Q-Network)法を用いたリバーシの強化学習を行った 報告書である。各局面において置ける箇所にランダムに手を打つプレイヤーと、DQN 手法を用い て手を打つプレイヤーを対戦させ、ハイパーパラメータの変化によって勝率がどう変化するか実 験を行った。実験に用いる DQN のエージェントの学習環境となるリバーシは、一般的な平面のリ バーシに高さを追加した 3 次元のリバーシ である。ハイパーパラメータには、バッチサイズ、e の 下限値を変化させて、適当なバッチサイズで学習させた場合に、e の下限値を減らすことで勝率が 伸びた。
## 環境
- macOS Catalina 10.15.7
- CPU: Intel Core i5
- 言語: Python 3.7.1
- 使用ライブラリ 
    - chainer 7.7.0 
    - numpy 1.18.1


## 実行方法
次のようにpythonで`main.py`を実行する。
```
python main.py
```
## 開発者
船附海斗 e185732@ie.u-ryukyu.ac.jp  
タカラジョシュア隼 e185734@ie.u-ryukyu.ac.jp  
大城祐介 e185743@ie.u-ryukyu.ac.jp  
長濱北斗 e185751@ie.u-ryukyu.ac.jp  
